{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-2535a41d45ec>, line 81)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-2535a41d45ec>\"\u001b[1;36m, line \u001b[1;32m81\u001b[0m\n\u001b[1;33m    print \"Epoch: %d, Loss: %.8f, Time: %.4fs\" % (epoch, np.mean( err ), time.clock()-t0 )\u001b[0m\n\u001b[1;37m                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "n_hidden = 10\n",
    "n_in = 10\n",
    "n_out = 10\n",
    "n_samples = 300\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return  1 - np.tanh(x)**2\n",
    "\n",
    "def train(x, t, V, W, bv, bw):\n",
    "\n",
    "    # forward\n",
    "    A = np.dot(x, V) + bv\n",
    "    Z = np.tanh(A)\n",
    "\n",
    "    B = np.dot(Z, W) + bw\n",
    "    Y = sigmoid(B)\n",
    "\n",
    "    # backward\n",
    "    Ew = Y - t\n",
    "    Ev = tanh_prime(A) * np.dot(W, Ew)\n",
    "\n",
    "    dW = np.outer(Z, Ew)\n",
    "    dV = np.outer(x, Ev)\n",
    "\n",
    "    loss = -np.mean ( t * np.log(Y) + (1 - t) * np.log(1 - Y) )\n",
    "\n",
    "    # Note that we use error for each layer as a gradient\n",
    "    # for biases\n",
    "\n",
    "    return  loss, (dV, dW, Ev, Ew)\n",
    "\n",
    "def predict(x, V, W, bv, bw):\n",
    "    A = np.dot(x, V) + bv\n",
    "    B = np.dot(np.tanh(A), W) + bw\n",
    "    return (sigmoid(B) > 0.5).astype(int)\n",
    "\n",
    "# Setup initial parameters\n",
    "# Note that initialization is cruxial for first-order methods!\n",
    "\n",
    "V = np.random.normal(scale=0.1, size=(n_in, n_hidden))\n",
    "W = np.random.normal(scale=0.1, size=(n_hidden, n_out))\n",
    "\n",
    "bv = np.zeros(n_hidden)\n",
    "bw = np.zeros(n_out)\n",
    "\n",
    "params = [V,W,bv,bw]\n",
    "\n",
    "# Generate some data\n",
    "\n",
    "X = np.random.binomial(1, 0.5, (n_samples, n_in))\n",
    "T = X ^ 1\n",
    "\n",
    "# Train\n",
    "for epoch in range(100):\n",
    "    err = []\n",
    "    upd = [0]*len(params)\n",
    "\n",
    "    t0 = time.clock()\n",
    "    for i in range(X.shape[0]):\n",
    "        loss, grad = train(X[i], T[i], *params)\n",
    "\n",
    "        for j in range(len(params)):\n",
    "            params[j] -= upd[j]\n",
    "\n",
    "        for j in range(len(params)):\n",
    "            upd[j] = learning_rate * grad[j] + momentum * upd[j]\n",
    "\n",
    "        err.append( loss )\n",
    "\n",
    "    print \"Epoch: %d, Loss: %.8f, Time: %.4fs\" % (epoch, np.mean( err ), time.clock()-t0 )\n",
    "\n",
    "# Try to predict something\n",
    "\n",
    "x = np.random.binomial(1, 0.5, n_in)\n",
    "print \"XOR prediction:\"\n",
    "print x\n",
    "print predict(x, *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
